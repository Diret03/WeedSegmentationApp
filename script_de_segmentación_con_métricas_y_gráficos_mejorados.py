# -*- coding: utf-8 -*-
"""Script de Segmentación con Métricas y Gráficos Mejorados

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RgBFrWvobh0ZV5oZyg-sn_ujRCPUZTDP
"""

# -*- coding: utf-8 -*-
"""
weed_segmentation_0.6_metrics_and_plots.py

Este script es una fusión de dos lógicas:
1.  Toma el robusto pipeline de entrenamiento de 'weed_segmentation_0.4' (pérdida combinada,
    scheduler, early stopping)
2.  Incorpora la arquitectura de modelo avanzada y la lógica de preprocesamiento  (recorte inteligente,
    FPN con atención y supervisión profunda).
3.  Añade el seguimiento de Dice Loss y Mean Dice Coefficient.
4.  Genera y guarda los gráficos de resultados de forma individual.
"""

import os
import time
from datetime import datetime
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau
import timm
import cv2
import albumentations as A
from tqdm import tqdm
from albumentations.pytorch import ToTensorV2
from sklearn.metrics import confusion_matrix
from tqdm import tqdm
import albumentations as A
from albumentations.pytorch import ToTensorV2
import cv2
from sklearn.metrics import confusion_matrix

# ==============================================================================
# CONFIGURACIÓN
# ==============================================================================
# Ajusta esta ruta a la ubicación de tu dataset
try:
    from google.colab import drive
    drive.mount('/content/drive')
    BASE_DATA_PATH = '/content/drive/My Drive/Datasets/Balanced_Augmented_Redistributed'
    OUTPUT_BASE_DIR = '/content/drive/My Drive/Models/Output'
    MODEL_SAVE_DIR = '/content/drive/My Drive/Models'
    print("Google Drive montado.")
except (ImportError, ModuleNotFoundError):
    BASE_DATA_PATH = '../Datasets/Balanced_Augmented_Redistributed'
    OUTPUT_BASE_DIR = '../Output'
    MODEL_SAVE_DIR = '../Models'
    print("Entorno local detectado.")

print(f"La ruta base del dataset es: {BASE_DATA_PATH}")
print(f"Los resultados se guardarán en: {OUTPUT_BASE_DIR}")
print(f"Los modelos se guardarán en: {MODEL_SAVE_DIR}")


IMAGE_DIR_NAME = 'images'
MASK_DIR_NAME = 'masks'

IMG_HEIGHT = 256
IMG_WIDTH = 256
N_CLASSES = 6

# Hiperparámetros de entrenamiento - OPTIMIZADOS
LEARNING_RATE = 3e-5        # MÁS BAJO para compensar batch grande
BATCH_SIZE = 32
NUM_EPOCHS = 200
PATIENCE = 15               # Mantener paciencia original
WEIGHT_DECAY = 3e-5         # MÁS BAJO para menos regularización

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Usando el dispositivo: {DEVICE}")

# ==============================================================================
# FUNCIÓN DE RECORTE INTELIGENTE
# ==============================================================================
def crop_around_classes(
    image: np.ndarray,
    mask: np.ndarray,
    classes_to_find: list[int] = [1, 2, 3, 4, 5],
    margin: int = 10
) -> tuple[np.ndarray, np.ndarray]:
    """
    Recorta un rectángulo alrededor de todos los píxeles que pertenecen a las
    clases especificadas en la máscara.
    """
    if mask.ndim == 3:
        mask_2d = mask.squeeze()
    else:
        mask_2d = mask

    is_class_present = np.isin(mask_2d, classes_to_find)
    ys, xs = np.where(is_class_present)

    if ys.size == 0:
        return image, mask

    y_min, y_max = ys.min(), ys.max()
    x_min, x_max = xs.min(), xs.max()

    y0 = max(0, y_min - margin)
    y1 = min(image.shape[0], y_max + margin + 1)
    x0 = max(0, x_min - margin)
    x1 = min(image.shape[1], x_max + margin + 1)

    cropped_image = image[y0:y1, x0:x1]
    cropped_mask = mask[y0:y1, x0:x1]

    return cropped_image, cropped_mask

# ==============================================================================
# AUMENTO DE DATOS
# ==============================================================================
def get_transforms(split='train'):
    if split == 'train':
        return A.Compose([
            A.Resize(IMG_HEIGHT, IMG_WIDTH, interpolation=cv2.INTER_NEAREST),
            A.HorizontalFlip(p=0.5),
            A.VerticalFlip(p=0.3),  # Reducido de 0.5
            A.RandomRotate90(p=0.3),  # Reducido de 0.5
            A.ShiftScaleRotate(
                shift_limit=0.05,     # Reducido de 0.0625
                scale_limit=0.05,     # Reducido de 0.1
                rotate_limit=30,      # Reducido de 45
                p=0.4                 # Reducido de 0.5
            ),
            A.RandomBrightnessContrast(
                brightness_limit=0.15,  # Reducido de 0.2
                contrast_limit=0.15,    # Reducido de 0.2
                p=0.4                   # Reducido de 0.5
            ),
            A.ElasticTransform(
                p=0.3,                  # Reducido de 0.5
                alpha=100,              # Reducido de 120
                sigma=100 * 0.05
            ),
            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
            ToTensorV2(),
        ])
    else:
        return A.Compose([
            A.Resize(IMG_HEIGHT, IMG_WIDTH, interpolation=cv2.INTER_NEAREST),
            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
            ToTensorV2(),
        ])

# ==============================================================================
# DATASET
# ==============================================================================
class PotatoWeedDataset(Dataset):
    def __init__(self, base_path, split='train', transform=None):
        self.split_path = os.path.join(base_path, split)
        self.image_dir = os.path.join(self.split_path, IMAGE_DIR_NAME)
        self.mask_dir = os.path.join(self.split_path, MASK_DIR_NAME)
        self.transform = transform

        all_files = os.listdir(self.image_dir)
        self.image_filenames = sorted([f for f in all_files if f.lower().endswith(('.jpg', '.png', '.jpeg'))])
        valid_filenames = []
        for img_fn in self.image_filenames:
            base_name, _ = os.path.splitext(img_fn)
            mask_fn = f"{base_name}_mask.png"
            if os.path.exists(os.path.join(self.mask_dir, mask_fn)):
                valid_filenames.append(img_fn)
        self.image_filenames = valid_filenames
        print(f"Encontrados {len(self.image_filenames)} pares de imagen-máscara en el split '{split}'.")
        if not self.image_filenames:
            raise IOError(f"No se encontraron pares en {self.split_path}.")

    def __len__(self):
        return len(self.image_filenames)

    def __getitem__(self, idx):
        img_name = self.image_filenames[idx]
        img_path = os.path.join(self.image_dir, img_name)
        base_name, _ = os.path.splitext(img_name)
        mask_name = f"{base_name}_mask.png"
        mask_path = os.path.join(self.mask_dir, mask_name)

        image = np.array(Image.open(img_path).convert("RGB"))
        mask = np.array(Image.open(mask_path).convert("L"))

        image_cropped, mask_cropped = crop_around_classes(image, mask)

        if self.transform:
            augmented = self.transform(image=image_cropped, mask=mask_cropped)
            image = augmented['image']
            mask = augmented['mask']

        return image, mask.long()

# ==============================================================================
# ARQUITECTURA DEL MODELO (WeedSegmenterFPN)
# ==============================================================================
class ChannelAttention(nn.Module):
    def __init__(self, in_channels, reduction_ratio=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        self.fc = nn.Sequential(
            nn.Conv2d(in_channels, in_channels // reduction_ratio, 1, bias=False),
            nn.ReLU(),
            nn.Conv2d(in_channels // reduction_ratio, in_channels, 1, bias=False)
        )
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.fc(self.avg_pool(x))
        max_out = self.fc(self.max_pool(x))
        return self.sigmoid(avg_out + max_out)

class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        padding = kernel_size // 2
        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x_cat = torch.cat([avg_out, max_out], dim=1)
        return self.sigmoid(self.conv1(x_cat))

class AttentionModule(nn.Module):
    def __init__(self, in_channels, reduction_ratio=16, kernel_size=7):
        super(AttentionModule, self).__init__()
        self.channel_attention = ChannelAttention(in_channels, reduction_ratio)
        self.spatial_attention = SpatialAttention(kernel_size)

    def forward(self, x):
        x = x * self.channel_attention(x)
        x = x * self.spatial_attention(x)
        return x

class ASPPConv(nn.Sequential):
    def __init__(self, in_channels, out_channels, dilation):
        super(ASPPConv, self).__init__(
            nn.Conv2d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU()
        )

class ASPPPooling(nn.Sequential):
    def __init__(self, in_channels, out_channels):
        super(ASPPPooling, self).__init__(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(in_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels),
            nn.ReLU()
        )

    def forward(self, x):
        size = x.shape[-2:]
        x = super(ASPPPooling, self).forward(x)
        return F.interpolate(x, size=size, mode='bilinear', align_corners=False)

class ASPP(nn.Module):
    def __init__(self, in_channels, atrous_rates, out_channels=256):
        super(ASPP, self).__init__()
        modules = [
            nn.Sequential(nn.Conv2d(in_channels, out_channels, 1, bias=False), nn.BatchNorm2d(out_channels), nn.ReLU())
        ]
        for rate in atrous_rates:
            modules.append(ASPPConv(in_channels, out_channels, rate))
        modules.append(ASPPPooling(in_channels, out_channels))
        self.convs = nn.ModuleList(modules)
        self.project = nn.Sequential(
            nn.Conv2d(len(self.convs) * out_channels, out_channels, 1, bias=False),
            nn.BatchNorm2d(out_channels), nn.ReLU(), nn.Dropout(0.5)
        )

    def forward(self, x):
        res = [conv(x) for conv in self.convs]
        res = torch.cat(res, dim=1)
        return self.project(res)

class DecoderBlock(nn.Module):
    def __init__(self, in_channels_skip, in_channels_up, out_channels, use_attention=True):
        super(DecoderBlock, self).__init__()
        self.upsample = nn.ConvTranspose2d(in_channels_up, in_channels_up, kernel_size=2, stride=2)
        total_in_channels = in_channels_skip + in_channels_up
        self.use_attention = use_attention
        if use_attention:
            self.attention = AttentionModule(in_channels=in_channels_skip)

        self.conv_fuse = nn.Sequential(
            nn.Conv2d(total_in_channels, out_channels, 3, padding=1, bias=False), nn.BatchNorm2d(out_channels), nn.ReLU(),
            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False), nn.BatchNorm2d(out_channels), nn.ReLU(),
            nn.Dropout(0.3)
        )

    def forward(self, x_skip, x_up):
        x_up = self.upsample(x_up)
        if self.use_attention:
            x_skip_att = self.attention(x_skip)
        else:
            x_skip_att = x_skip
        x_concat = torch.cat([x_up, x_skip_att], dim=1)
        return self.conv_fuse(x_concat)

class WeedSegmenterFPN(nn.Module):
    def __init__(self, num_classes=N_CLASSES):
        super(WeedSegmenterFPN, self).__init__()
        self.training = True

        self.backbone = timm.create_model(
            'tf_efficientnetv2_s.in21k', pretrained=True, features_only=True, out_indices=(0, 1, 2, 3)
        )
        backbone_channels = self.backbone.feature_info.channels()

        self.aspp = ASPP(in_channels=backbone_channels[3], atrous_rates=(6, 12, 18), out_channels=256)

        decoder_out_channels = [128, 64, 48]
        self.decoder_block3 = DecoderBlock(backbone_channels[2], 256, decoder_out_channels[0])
        self.decoder_block2 = DecoderBlock(backbone_channels[1], decoder_out_channels[0], decoder_out_channels[1])
        self.decoder_block1 = DecoderBlock(backbone_channels[0], decoder_out_channels[1], decoder_out_channels[2])

        self.segmentation_head = nn.Sequential(
            nn.Conv2d(decoder_out_channels[2], 32, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(32), nn.ReLU(),
            nn.Conv2d(32, num_classes, kernel_size=1)
        )

        self.aux_head_3 = nn.Conv2d(decoder_out_channels[0], num_classes, 1)
        self.aux_head_2 = nn.Conv2d(decoder_out_channels[1], num_classes, 1)
        self.final_upsample = nn.UpsamplingBilinear2d(scale_factor=2)

    def forward(self, x):
        img_size = x.shape[-2:]
        features = self.backbone(x)
        aspp_output = self.aspp(features[3])
        decoder_out3 = self.decoder_block3(x_skip=features[2], x_up=aspp_output)
        decoder_out2 = self.decoder_block2(x_skip=features[1], x_up=decoder_out3)
        decoder_out1 = self.decoder_block1(x_skip=features[0], x_up=decoder_out2)
        logits = self.segmentation_head(decoder_out1)
        final_logits = self.final_upsample(logits)

        if self.training:
            aux3 = F.interpolate(self.aux_head_3(decoder_out3), size=img_size, mode='bilinear', align_corners=False)
            aux2 = F.interpolate(self.aux_head_2(decoder_out2), size=img_size, mode='bilinear', align_corners=False)
            return final_logits, aux3, aux2
        return final_logits

# ==============================================================================
# FUNCIONES DE PÉRDIDA Y MÉTRICAS
# ==============================================================================
class DiceLoss(nn.Module):
    def __init__(self, smooth=1e-6):
        super(DiceLoss, self).__init__()
        self.smooth = smooth

    def forward(self, inputs, targets):
        inputs = F.softmax(inputs, dim=1)
        targets_one_hot = F.one_hot(targets, num_classes=inputs.shape[1]).permute(0, 3, 1, 2).float()
        intersection = torch.sum(inputs * targets_one_hot, dim=(2, 3))
        cardinality = torch.sum(inputs + targets_one_hot, dim=(2, 3))
        dice_score = (2. * intersection + self.smooth) / (cardinality + self.smooth)
        return (1 - dice_score).mean()

class CombinedLoss(nn.Module):
    def __init__(self, alpha=0.6, beta=0.4, gamma=2.5, class_weights=None):  # Parámetros optimizados
        super(CombinedLoss, self).__init__()
        self.alpha = alpha
        self.beta = beta
        self.focal_loss_fn = nn.CrossEntropyLoss(weight=class_weights)
        self.gamma = gamma  # Más agresivo para clases difíciles
        self.dice_loss_fn = DiceLoss(smooth=1e-5)  # Smooth más pequeño

    def forward(self, outputs, targets):
        ce_loss = self.focal_loss_fn(outputs, targets)
        pt = torch.exp(-ce_loss)
        focal = ((1 - pt) ** self.gamma * ce_loss).mean()
        dice = self.dice_loss_fn(outputs, targets)
        return self.alpha * focal + self.beta * dice

def compute_per_class_iou_from_confmat(conf_mat):
    num_classes = conf_mat.shape[0]
    ious = []
    for i in range(num_classes):
        tp = conf_mat[i, i]
        fp = conf_mat[:, i].sum() - tp
        fn = conf_mat[i, :].sum() - tp
        union = tp + fp + fn
        ious.append(tp / union if union > 0 else float('nan'))
    return ious

# ==============================================================================
# FUNCIONES DE ENTRENAMIENTO Y VALIDACIÓN
# ==============================================================================
def train_one_epoch(model, dataloader, optimizer, criterion, device, scheduler, epoch_num, num_epochs, warmup_epochs):
    model.train()
    model.training = True
    total_loss = 0.0

    if epoch_num < warmup_epochs:
        lr_scale = (epoch_num + 1) / warmup_epochs
        for param_group in optimizer.param_groups:
            param_group['lr'] = LEARNING_RATE * lr_scale

    progress_bar = tqdm(dataloader, desc=f"Epoch {epoch_num+1}/{num_epochs} [Training]")
    for images, masks in progress_bar:
        images, masks = images.to(device), masks.to(device, non_blocking=True)
        optimizer.zero_grad()

        main_output, aux_output1, aux_output2 = model(images)
        loss_main = criterion(main_output, masks)
        loss_aux1 = criterion(aux_output1, masks)
        loss_aux2 = criterion(aux_output2, masks)
        loss = loss_main + 0.4 * loss_aux1 + 0.2 * loss_aux2

        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        progress_bar.set_postfix(loss=loss.item())

    avg_loss = total_loss / len(dataloader)
    # Nota: El scheduler ahora se llama desde el bucle principal con val_miou
    return avg_loss

def validate_model(model, dataloader, criterion, device, num_classes, epoch_num, num_epochs):
    model.eval()
    model.training = False
    total_val_loss = 0.0
    total_dice_loss = 0.0
    all_preds, all_masks = [], []
    num_batches = 0

    # Instanciamos una función de pérdida Dice para la métrica
    dice_loss_fn = DiceLoss().to(device)

    progress_bar = tqdm(dataloader, desc=f"Epoch {epoch_num+1}/{num_epochs} [Validation]")
    with torch.no_grad():
        for images, masks in progress_bar:
            images, masks = images.to(device), masks.to(device, non_blocking=True)
            outputs = model(images)

            # Calcular pérdidas
            val_loss = criterion(outputs, masks)
            dice_loss = dice_loss_fn(outputs, masks)

            total_val_loss += val_loss.item()
            total_dice_loss += dice_loss.item()

            # Preparar para métricas de segmentación
            preds = torch.argmax(outputs, dim=1)
            all_preds.append(preds.cpu().numpy().ravel())
            all_masks.append(masks.cpu().numpy().ravel())
            num_batches += 1
            progress_bar.set_postfix(val_loss=val_loss.item(), dice_loss=dice_loss.item())

    # Calcular métricas agregadas
    y_true = np.concatenate(all_masks)
    y_pred = np.concatenate(all_preds)
    conf_mat = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))
    per_class_iou = compute_per_class_iou_from_confmat(conf_mat)
    valid_ious = [iou for iou in per_class_iou if not np.isnan(iou)]
    avg_miou = np.mean(valid_ious) if valid_ious else 0.0

    avg_val_loss = total_val_loss / num_batches
    avg_dice_loss = total_dice_loss / num_batches
    mean_dice_coeff = 1 - avg_dice_loss

    return avg_val_loss, avg_dice_loss, mean_dice_coeff, avg_miou, per_class_iou, conf_mat

# ==============================================================================
# SCRIPT PRINCIPAL
# ==============================================================================
def main():
    def calculate_class_weights(dataset, num_classes):
        print("Calculando pesos de clase...")
        class_counts = torch.zeros(num_classes)
        loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)
        for _, masks in tqdm(loader, desc="Contando píxeles por clase"):
            for i in range(num_classes):
                class_counts[i] += (masks == i).sum()
        total_pixels = class_counts.sum()
        class_weights = total_pixels / (num_classes * class_counts)
        print("Pesos de clase calculados:", class_weights)
        return class_weights.to(DEVICE)

    weight_transform = A.Compose([A.Resize(IMG_HEIGHT, IMG_WIDTH, interpolation=cv2.INTER_NEAREST), ToTensorV2()])
    weight_dataset = PotatoWeedDataset(BASE_DATA_PATH, split='train', transform=weight_transform)
    class_weights = calculate_class_weights(weight_dataset, N_CLASSES)

    train_dataset = PotatoWeedDataset(BASE_DATA_PATH, split='train', transform=get_transforms('train'))
    val_dataset = PotatoWeedDataset(BASE_DATA_PATH, split='val', transform=get_transforms('val'))
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)

    model = WeedSegmenterFPN(num_classes=N_CLASSES).to(DEVICE)
    print("Modelo 'WeedSegmenterFPN' cargado.")

    criterion = CombinedLoss(class_weights=class_weights).to(DEVICE)
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
    warmup_epochs = 5
    
    # Scheduler mejorado - más adaptativo
    scheduler = ReduceLROnPlateau(
        optimizer, 
        mode='max',           # Maximizar mIoU
        factor=0.5,          # Reducir LR a la mitad
        patience=8,          # Esperar 8 épocas sin mejora
        verbose=True,
        min_lr=1e-7
    )

    best_val_miou = 0.0
    best_dice_loss = float('inf')
    best_mean_dice_coeff = 0.0
    patience_counter = 0
    start_time = time.time()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_name_template = f"weed_segmenter_fpn_model_{timestamp}"

    # Crear directorios para guardar modelos y resultados
    output_dir = os.path.join(OUTPUT_BASE_DIR, model_name_template)
    os.makedirs(output_dir, exist_ok=True)
    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)

    # Historial de métricas
    train_losses, val_losses = [], []
    val_dice_losses, val_mean_dice_coeffs = [], []
    val_mious = []
    per_class_ious_history = [[] for _ in range(N_CLASSES)]
    last_conf_mat = None

    for epoch in range(NUM_EPOCHS):
        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE, scheduler, epoch, NUM_EPOCHS, warmup_epochs)
        val_loss, val_dice_loss, mean_dice_coeff, val_miou, pc_iou, conf_mat = validate_model(model, val_loader, criterion, DEVICE, N_CLASSES, epoch, NUM_EPOCHS)

        # Guardar historial
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        val_dice_losses.append(val_dice_loss)
        val_mean_dice_coeffs.append(mean_dice_coeff)
        val_mious.append(val_miou)
        for i, iou in enumerate(pc_iou):
            per_class_ious_history[i].append(iou)
        last_conf_mat = conf_mat

        current_lr = optimizer.param_groups[0]['lr']
        
        # Imprimir IoU por clase
        class_names = ['background', 'Cow-tongue', 'Dandelion', 'Kikuyo', 'Other', 'Potato']
        print(f"Epoch {epoch+1}/{NUM_EPOCHS} -> Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val mIoU: {val_miou:.4f} | Mean Dice: {mean_dice_coeff:.4f} | LR: {current_lr:.6f}")
        
        # Imprimir IoU por clase individualmente
        print("IoU por clase:")
        for i, (class_name, iou) in enumerate(zip(class_names, pc_iou)):
            if not np.isnan(iou):
                print(f"  {class_name}: {iou:.4f}")
            else:
                print(f"  {class_name}: N/A")
        print()  # Línea en blanco para mejor legibilidad

        if val_miou > best_val_miou:
            best_val_miou = val_miou
            patience_counter = 0
            save_path = os.path.join(MODEL_SAVE_DIR, f"{model_name_template}_best.pth")
            torch.save(model.state_dict(), save_path)
            print(f"Nuevo mejor mIoU ({best_val_miou:.4f}). Modelo guardado en {save_path}")
        else:
            patience_counter += 1
            
        # Actualizar mejores valores de Dice Loss y Mean Dice Coefficient
        if val_dice_loss < best_dice_loss:
            best_dice_loss = val_dice_loss
            
        if mean_dice_coeff > best_mean_dice_coeff:
            best_mean_dice_coeff = mean_dice_coeff

        # Llamar al scheduler con mIoU después del warmup
        if epoch >= warmup_epochs:
            scheduler.step(val_miou)

        if patience_counter >= PATIENCE:
            print(f"Parada temprana (Early stopping) en la época {epoch+1}")
            break

    total_training_time = time.time() - start_time
    print(f"\nEntrenamiento finalizado. Mejor mIoU de validación: {best_val_miou:.4f}")
    print(f"Mejor Dice Loss: {best_dice_loss:.4f}")
    print(f"Mejor Mean Dice Coefficient: {best_mean_dice_coeff:.4f}")
    print(f"Tiempo total de entrenamiento: {total_training_time:.2f} segundos")
    print(f"Resultados guardados en: {output_dir}")
    
    # Mostrar IoU final por clase
    print("\nIoU final por clase:")
    class_names = ['background', 'Cow-tongue', 'Dandelion', 'Kikuyo', 'Other', 'Potato']
    final_per_class_iou = compute_per_class_iou_from_confmat(last_conf_mat)
    for i, (class_name, iou) in enumerate(zip(class_names, final_per_class_iou)):
        if not np.isnan(iou):
            print(f"  {class_name}: {iou:.4f}")
        else:
            print(f"  {class_name}: N/A")
    print()

    # --- VISUALIZACIÓN Y GUARDADO DE GRÁFICOS ---
    epochs = range(1, len(train_losses) + 1)
    class_names = ['background', 'Cow-tongue', 'Dandelion', 'Kikuyo', 'Other', 'Potato']

    # Gráfico 1: Curvas de pérdida
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, train_losses, label='Train Loss')
    plt.plot(epochs, val_losses, label='Val Loss')
    plt.xlabel('Época'); plt.ylabel('Pérdida'); plt.legend(); plt.title('Pérdida de Entrenamiento y Validación vs. Época'); plt.grid(True)
    plt.savefig(os.path.join(output_dir, "plot_losses.png"))
    plt.show()

    # Gráfico 2: Curva de mIoU
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, val_mious, label='Val mIoU', color='green')
    plt.xlabel('Época'); plt.ylabel('mIoU'); plt.title('Validación mIoU vs. Época'); plt.grid(True)
    plt.savefig(os.path.join(output_dir, "plot_miou.png"))
    plt.show()

    # Gráfico 3: Curva de Dice Loss
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, val_dice_losses, label='Val Dice Loss', color='orange')
    plt.xlabel('Época'); plt.ylabel('Dice Loss'); plt.title('Validación Dice Loss vs. Época'); plt.grid(True)
    plt.savefig(os.path.join(output_dir, "plot_dice_loss.png"))
    plt.show()

    # Gráfico 4: Curva de Mean Dice Coefficient
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, val_mean_dice_coeffs, label='Val Mean Dice Coeff', color='purple')
    plt.xlabel('Época'); plt.ylabel('Mean Dice Coefficient'); plt.title('Validación Mean Dice Coefficient vs. Época'); plt.grid(True)
    plt.savefig(os.path.join(output_dir, "plot_mean_dice_coeff.png"))
    plt.show()

    # Gráfico 5: Curvas de IoU por clase
    plt.figure(figsize=(12, 7))
    for idx, cls in enumerate(class_names):
        plt.plot(epochs, per_class_ious_history[idx], label=cls) # Corrección aplicada aquí
    plt.xlabel('Época'); plt.ylabel('IoU'); plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left'); plt.title('IoU por Clase vs. Época'); plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, "plot_per_class_iou.png"))
    plt.show()

    # Gráfico 6: Matriz de confusión
    if last_conf_mat is not None:
        plt.figure(figsize=(10, 8))
        plt.imshow(last_conf_mat, interpolation='nearest', cmap=plt.cm.Blues)
        plt.title('Matriz de Confusión del Último Epoch')
        plt.colorbar()
        tick_marks = np.arange(len(class_names))
        plt.xticks(tick_marks, class_names, rotation=45, ha="right")
        plt.yticks(tick_marks, class_names)
        thresh = last_conf_mat.max() / 2.
        for i in range(last_conf_mat.shape[0]):
            for j in range(last_conf_mat.shape[1]):
                plt.text(j, i, format(last_conf_mat[i, j], 'd'), ha="center", va="center",
                         color="white" if last_conf_mat[i, j] > thresh else "black")
        plt.ylabel('Etiqueta Verdadera')
        plt.xlabel('Etiqueta Predicha')
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, "plot_confusion_matrix.png"))
        plt.show()


if __name__ == "__main__":
    main()